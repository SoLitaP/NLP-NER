{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T15:18:37.622459Z",
     "start_time": "2024-12-17T15:18:32.456257Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset as HFDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Preprocessing utilities\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tag_seq = self.tags[idx]\n",
    "\n",
    "        # Padding and converting to indices\n",
    "        sentence_padded = [self.word2idx.get(word, self.word2idx[\"<UNK>\"]) for word in sentence]\n",
    "        tag_padded = [self.tag2idx[tag] for tag in tag_seq]\n",
    "\n",
    "        sentence_padded = sentence_padded[:self.max_len] + [self.word2idx[\"<PAD>\"]] * (\n",
    "                    self.max_len - len(sentence_padded))\n",
    "        tag_padded = tag_padded[:self.max_len] + [self.tag2idx[\"<PAD>\"]] * (self.max_len - len(tag_padded))\n",
    "\n",
    "        return torch.tensor(sentence_padded, dtype=torch.long), torch.tensor(tag_padded, dtype=torch.long)\n",
    "\n",
    "\n",
    "# BIO Rule Enforcement\n",
    "def enforce_bio_rules(tags):\n",
    "    \"\"\"Ensure 'B' tag always precedes 'I' tag.\"\"\"\n",
    "    corrected_tags = []\n",
    "    prev_tag = 'O'\n",
    "\n",
    "    for tag in tags:\n",
    "        if tag == 'I' and prev_tag not in {'B', 'I'}:\n",
    "            corrected_tags.append('B')\n",
    "        else:\n",
    "            corrected_tags.append(tag)\n",
    "        prev_tag = tag\n",
    "\n",
    "    return corrected_tags\n",
    "\n",
    "\n",
    "# Model Definition\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(dataset, max_len):\n",
    "    sentences = dataset[\"tokens\"]\n",
    "    tags = dataset[\"tags\"]\n",
    "\n",
    "    # Enforce BIO rules on the data\n",
    "    tags = [enforce_bio_rules(tag_seq) for tag_seq in tags]\n",
    "\n",
    "    vocab = {word for sentence in sentences for word in sentence}\n",
    "    tags_set = {tag for tag_seq in tags for tag in tag_seq}\n",
    "\n",
    "    word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    word2idx[\"<UNK>\"] = 1\n",
    "\n",
    "    tag2idx = {tag: idx + 1 for idx, tag in enumerate(tags_set)}\n",
    "    tag2idx[\"<PAD>\"] = 0\n",
    "\n",
    "    return sentences, tags, word2idx, tag2idx\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sentences, tags in train_loader:\n",
    "        sentences, tags = sentences.to(device), tags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sentences)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(outputs, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, tag2idx, idx2tag, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in test_loader:\n",
    "            sentences, tags = sentences.to(device), tags.to(device)\n",
    "            outputs = model(sentences)\n",
    "            predicted = torch.argmax(outputs, dim=-1)\n",
    "            for true, pred in zip(tags, predicted):\n",
    "                true = true.cpu().numpy()\n",
    "                pred = pred.cpu().numpy()\n",
    "                for t, p in zip(true, pred):\n",
    "                    if t != tag2idx[\"<PAD>\"]:\n",
    "                        y_true.append(idx2tag[t])\n",
    "                        y_pred.append(idx2tag[p])\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Accuracy and F1 Score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    labels = [tag for tag in tag2idx if tag != \"<PAD>\"]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T15:18:37.860954Z",
     "start_time": "2024-12-17T15:18:37.623478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Function to predict tags for a new sentence\n",
    "def predict_tags(tokens):\n",
    "    model.eval()\n",
    "    features = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in tokens]\n",
    "    features_padded = features + [word2idx[\"<PAD>\"]] * (MAX_LEN - len(features))\n",
    "    input_tensor = torch.tensor([features_padded], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_tags_idx = torch.argmax(output, dim=-1).cpu().numpy()[0]\n",
    "        predicted_tags = [idx2tag[idx] for idx in predicted_tags_idx[:len(tokens)]]\n",
    "\n",
    "    # Enforce BIO rules\n",
    "    corrected_tags = enforce_bio_rules(predicted_tags)\n",
    "    return list(zip(tokens, corrected_tags))\n",
    "\n",
    "\n",
    "# BIO Map\n",
    "BIO_MAP = {'O': 0, 'B': 1, 'I': 2}\n",
    "idx2tag = {idx: tag for tag, idx in BIO_MAP.items()}\n",
    "tag2idx = BIO_MAP\n",
    "\n",
    "# Dataset preparation\n",
    "train_hugging_face_dat = HFDataset.from_dict({\n",
    "    \"tokens\": train_df[\"tokens\"],\n",
    "    \"tags\": train_df[\"tags\"]\n",
    "})\n",
    "\n",
    "test_hugging_face_dat = HFDataset.from_dict({\n",
    "    \"tokens\": test_df[\"tokens\"],\n",
    "    \"tags\": test_df[\"tags\"]\n",
    "})\n",
    "\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "train_sentences, train_tags, word2idx, tag2idx = preprocess_data(train_hugging_face_dat, MAX_LEN)\n",
    "test_sentences, test_tags, _, _ = preprocess_data(test_hugging_face_dat, MAX_LEN)\n",
    "\n",
    "train_dataset = SequenceDataset(train_sentences, train_tags, word2idx, tag2idx, MAX_LEN)\n",
    "test_dataset = SequenceDataset(test_sentences, test_tags, word2idx, tag2idx, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMTagger(len(word2idx), len(tag2idx), EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "evaluate_model(model, test_loader, tag2idx, idx2tag, device)"
   ],
   "id": "18f9ca5ff63c156d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 25\u001B[0m\n\u001B[0;32m     21\u001B[0m tag2idx \u001B[38;5;241m=\u001B[39m BIO_MAP\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Dataset preparation\u001B[39;00m\n\u001B[0;32m     24\u001B[0m train_hugging_face_dat \u001B[38;5;241m=\u001B[39m HFDataset\u001B[38;5;241m.\u001B[39mfrom_dict({\n\u001B[1;32m---> 25\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mtrain_df\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: train_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     27\u001B[0m })\n\u001B[0;32m     29\u001B[0m test_hugging_face_dat \u001B[38;5;241m=\u001B[39m HFDataset\u001B[38;5;241m.\u001B[39mfrom_dict({\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: test_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: test_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     32\u001B[0m })\n\u001B[0;32m     34\u001B[0m MAX_LEN \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Predict example\n",
    "tokens = [\"This\", \"is\", \"natural\", \"language\", \"processing\"]\n",
    "predicted_tags = predict_tags(tokens)\n",
    "\n",
    "for word, tag in predicted_tags:\n",
    "    print(f\"{word}: {tag}\")\n"
   ],
   "id": "a898f1eebfb2560"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
